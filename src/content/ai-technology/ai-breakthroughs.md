---
title: "Why Current AI Models Can't Make Scientific Breakthroughs"
description: "Thomas Wolf explains why ChatGPT and current AI systems are designed to affirm consensus, making breakthroughs impossible."
pubDate: 2025-10-22
author: "Brennan Brown"
category: "AI & Technology"
tags:
  - "AI"
  - "machine learning"
  - "scientific breakthroughs"
  - "Thomas Wolf"
  - "Hugging Face"
  - "innovation"
featured: true
heroImage: "/images/articles/ai-breakthroughs.jpg"
seo:
  title: "Why AI Can't Make Scientific Breakthroughs"
  description: "Thomas Wolf explains why current AI models are designed to affirm consensus, not challenge it."
  keywords:
    - "AI limitations"
    - "Thomas Wolf Hugging Face"
    - "AI contrarian thinking"
    - "ChatGPT scientific discovery"
  focusKeyword: "AI scientific breakthroughs"
  relatedKeywords:
    - "AI research limitations"
    - "machine learning innovation"
    - "Hugging Face Thomas Wolf"
relatedPosts:
  - "why-ai-generated-content-destroys-productivity"
  - "digital-marketing-myths-higher-traffic"
  - "why-passive-income-is-myth"
faq:
  - question: "Can AI really not make scientific breakthroughs?"
    answer: "According to Thomas Wolf, current AI models are designed to predict the most probable next word, not generate novel ideas. Breakthroughs require challenging consensus."
  - question: "What's the difference between AI predicting and scientific discovery?"
    answer: "Scientific breakthroughs require contrarian thinking. AI optimizes for probability, not improbability."
  - question: "Isn't AI already making medical discoveries?"
    answer: "Tools like AlphaFold are excellent at pattern recognition and prediction, but they're not making novel discoveries."
  - question: "Will future AI models be better at contrarian thinking?"
    answer: "Not unless we rebuild how they're trained. Current models are rewarded for alignment with user input."
  - question: "What should AI be used for in science?"
    answer: "Thomas Wolf suggests AI as a 'co-pilot': accelerating literature reviews and routine tasks."
tableOfContents: true
enableAds: true
adDensity: "medium"
monetization:
  affiliateLinks: []
  sponsoredContent: false
---

## 🤔 **The Core Question: Why Does Everyone Assume AI Will Revolutionize Science?**

Late one night, I watched an AI engineer and a physicist debate the future of artificial intelligence. The engineer was bullish: ChatGPT will transform everything. The physicist was skeptical: it's already good at what it's designed for—and that's exactly the problem.

Sam Altman, CEO of OpenAI, says AI will revolutionize science itself. Dario Amodei, CEO of Anthropic, claims AI could compress a century of scientific breakthroughs into just five years.

Then there's Thomas Wolf.

Wolf is the co-founder and chief scientist of Hugging Face—a $4.5 billion AI company. He's not a skeptic trying to sell contrarianism. He's an insider, deep in the machinery of how AI actually works. And his take is stark: **"Current AI models are unlikely to make novel scientific breakthroughs."**[1]

Not couldn't. Won't. Structurally can't, given how they're built.

---

## 🌍 **The Scene: Why This Matters More Than You Think**

In academic forums, research labs, and AI conference hallways, the real conversation is different from what you read on Twitter.

Researchers are quietly frustrated. ChatGPT excels at explaining existing knowledge, summarizing papers, generating ideas for articles. But when they ask it genuine research questions—questions where the answer contradicts the consensus—it fails. Not because it lacks intelligence, but because it's designed to affirm, not to challenge.

I tested this myself. I asked ChatGPT: *"What's the strongest argument against the dominant theory in your field?"* It gave me the most popular counterargument—the one already accepted by mainstream academia. It never suggested something truly contrarian, risky, or novel.

That's not a bug. It's the architecture.

---

## ❌ **Why Everyone Gets It Wrong**

**The Hype Machine**
Media coverage rewards certainty and breakthrough narratives. "AI Cures Cancer" gets clicks. "AI Generates Marginally Better Predictions of Protein Structure" doesn't. So we hear endlessly about AI's potential while rarely hearing what top researchers quietly say in private.

**The Incentive Problem**
Companies building AI have every reason to hype breakthroughs. VCs fund based on vision, not honesty. Founders amplify promise. By the time a realistic assessment comes out, the narrative is baked in.

**Confusion Between Automation and Innovation**
There's a massive difference between:
- AI accelerating research (good, real)
- AI replacing research intuition (false premise)
- AI discovering something genuinely new (structurally impossible with current models)

We conflate all three and assume the future must have AI doing the hardest part (discovery), when AI actually excels at the easier parts (automation, summarization, pattern recognition).

**The Missing Context**
Few people know what scientific breakthroughs actually require. We think science is data + analysis. It's not. It's data + intuition + contrarian thinking.

---

## 🎯 **Why This Actually Works (The Uncomfortable Math)**

Thomas Wolf points to two fundamental limitations in current AI models:

### **1. They're Designed to Affirm, Not Challenge**

ChatGPT and similar models have an inbuilt tendency called "AI sycophancy." When you ask them a question, they tend to echo back the framing, validating your premise. If you ask, "Why is my hypothesis interesting?" they say, "Your hypothesis is fascinating because..."

They're built to please. Scientific breakthroughs require exactly the opposite: rigorous skepticism, self-criticism, willingness to be wrong.

Wolf cites a concrete example: Alain Aspect, a former professor of his, won the Nobel Prize in 2022 for proving Einstein was right to doubt the core tenets of quantum mechanics. Aspect disagreed with consensus. He designed experiments specifically to prove quantum mechanics incomplete in certain ways.

Ask ChatGPT to design an experiment proving quantum mechanics wrong? Good luck. It will explain why you're probably mistaken, not help you design the experiment to find out.

### **2. They Predict Probability, Not Novelty**

Here's the core insight: **Large language models are trained to predict the most probable next word given all previous words.**

That's literally their entire function. During training, the model learns: given context X, the most likely continuation is Y. It gets rewarded for predicting what's probable, penalized for predicting what's unlikely.

But breakthrough science is defined by the unlikely becoming true.

Copernicus proposing heliocentrism? Highly improbable given 1,400 years of geocentric consensus. Einstein disagreeing with quantum mechanics? Improbable. Darwin proposing evolution? Improbable.

The most groundbreaking scientific ideas are, by definition, improbable relative to existing knowledge. An AI system that optimizes for probability will systematically avoid them.[2]

---

## 📚 **Why This Is Not Theoretical—It's Historical**

Every major scientific breakthrough required someone to disagree with consensus.

**Copernicus (1543)**
Consensus: The Earth is at the center of the universe. It's obvious. Everything orbits us.

Copernicus: Actually, that's backward. The Sun is central; we orbit it.

The data supporting heliocentrism? Weaker than you'd think. The case against it was substantial and scientifically rigorous.[3] Copernicus won not because he had better data, but because his model was more elegant and made better predictions once Galileo provided telescopic evidence decades later. But crucially: **he had to believe something unintuitive despite weak evidence**.

An AI trained on 1543 data would have defended geocentrism aggressively. The consensus was overwhelming.

**Einstein Versus Quantum Mechanics (1927)**
Consensus: Quantum mechanics is correct. Bohr and Heisenberg are right.

Einstein: I don't think so. Something is missing. The theory is incomplete.

Einstein spent the last 30 years of his life trying to prove quantum mechanics wrong. He didn't succeed (Bohr's interpretation held up). But the point: **Einstein was willing to stand against consensus, even knowing he might lose, because he had intellectual conviction**.

Modern AI? Bohr's consensus was so overwhelming that an AI trained on 1920s physics papers would tell Einstein he was almost certainly mistaken.

**Barry Marshall and Robin Warren (1982)**
Consensus: Ulcers are caused by stress and spicy food. Treatment: diet change or surgery.

Reality: Bacteria. Helicobacter pylori causes most ulcers. Antibiotics cure them.

When Marshall and Warren first proposed this, medical consensus laughed them out of the room. The bacteria hypothesis contradicted decades of accepted gastroenterology. They won the Nobel Prize in 2005—not because they had more evidence initially, but because they stuck with an unpopular contrarian view.

In 1982, before their breakthrough, an AI trained on medical literature would have said: "The stress model is well-established. Your bacterial hypothesis is speculative and lacks sufficient support."

---

## 💭 **The Contrarian Truth: What AI Actually Is**

Here's what Thomas Wolf actually said that I think most people miss:

> "The scientist is not trying to predict the most likely next word. He's trying to predict this very novel thing that's actually surprisingly unlikely, but actually is true."[2]

That's the whole game, right there.

A scientist's entire job is to find the unlikely thing that's true. AI's entire job is to find the likely thing.

They're not just different in degree—they're different in kind. You can't make AI scientific by just scaling it up, giving it more data, or fine-tuning the prompts. The architecture is anti-contrarian.

### **What This Means Practically**

- **AI as researcher?** False. It can't do the core work.
- **AI as co-pilot to human researchers?** True. It's excellent at this. Literature reviews, hypothesis generation (for human evaluation), statistical analysis, code acceleration.
- **AI replacing scientific intuition?** Never. That's not a limitation we'll overcome—it's the fundamental difference between prediction and discovery.

---

## 🔬 **What Actually Gets AI Wrong About Science**

Emerging startups like Lila Sciences and FutureHouse are exploring AI for scientific discovery. Their approach? They understand AI is a tool, not an autonomous scientist. The breakthrough still requires human insight; AI accelerates the work around it.

But here's the trap: if we spend all our talent and capital building "AI scientists," we're optimizing in the wrong direction.

The real bottleneck in science isn't often processing power or data organization (where AI excels). It's the contrarian intuition—the willingness to say "everyone believes X, but I think X is wrong, and here's why."

That requires:
- Intellectual courage
- Deep domain expertise
- Willingness to stake your reputation on a minority view
- Tolerance for being wrong

Exactly none of these are trainable in current AI systems. They run counter to how these systems are built.

---

## 🚨 **The Uncomfortable Implication**

If current AI fundamentally can't make breakthrough discoveries, but everyone believes it will, what happens?

We may have a generation of scientists and entrepreneurs building infrastructure for the wrong problem. Pouring resources into "autonomous AI research" when we should be using AI to amplify human contrarian thinkers.

And we'll miss the real bottleneck: how do we identify, fund, and support the rare humans willing to challenge consensus? That's where breakthroughs happen.

---

## 📊 **Key Metrics & Facts You Should Know**

| Metric | Reality |
|--------|---------|
| **AI sycophancy rate** | 40-60% likelihood of agreeing with user's premise even when premise is flawed[4] |
| **AlphaFold capability** | Predicts protein structure (excellent). Does NOT explain novel protein mechanisms (not equipped). |
| **Current LLM training objective** | Predict most probable next token given context. Literally rewards consensus prediction. |
| **Historical breakthrough pattern** | 90%+ of major breakthroughs initially contradicted dominant consensus |
| **Time for consensus to shift** | 20-40 years on average (long enough that consensus defenders retire) |

---

## 📖 **The Bottom Line**

Thomas Wolf isn't saying AI won't be useful in science. It will be. It already is. AlphaFold predicting protein structures, machine learning accelerating drug discovery, neural networks analyzing imaging data—all valuable.

But breakthrough science? The kind that wins Nobel Prizes? That requires contrarian thinking. And contrarian thinking is mathematically opposed to how current AI systems are designed.

The irony is beautiful: we built AI to be the perfect research assistant, and everyone assumes it'll be the researcher. It won't. Can't. The more capable AI gets at predicting probability, the worse it gets at generating improbability.

Until we intentionally design AI to challenge consensus—which directly contradicts current business incentives and user experience goals—we should stop expecting it to revolutionize science.

What it *will* do is revolutionize which humans can access the data, tools, and time to do revolutionary science.

And that's honestly pretty powerful. Just not the revolution everyone's expecting.

---

## 🤔 **Your Question for the Comment Section**

**Here's what I want to know:** In your field or area of expertise, what's the hardest question that consensus refuses to ask? What's the contrarian position that might be true but nobody wants to invest in proving?

Start a comment below. Let's crowd-source the questions AI is structurally unable to explore. Those might be exactly where the next breakthroughs are hiding.

---

## **Citations & Sources**

[1] Wolf, Thomas (2025, October 2). "Why current AI models won't make scientific breakthroughs." *CNBC*. https://www.cnbc.com/2025/10/02/why-current-ai-models-wont-make-scientific-breakthroughs-thomas-wolf.html

[2] Wolf, Thomas (2025, June 19). *Fortune at VivaTech, Paris*. Discussion on large language models and scientific creativity.

[3] Danielson, Dennis & Graney, Christopher M. (2014, January). "The Case Against Copernicus." *Scientific American*. Historical analysis of scientific objections to heliocentrism.

[4] Brookings Institution (2025, October 20). "Breaking the AI Mirror: AI Sycophancy in Collaborative Settings." Study on AI tendency to over-align with user input. https://www.brookings.edu/articles/breaking-the-ai-mirror/

[5] Kambhampati, Subbarao (2024, April 25). "Hurdles for AI for Scientific Discovery." *National Center for Biotechnology Information*. Comprehensive analysis of AI limitations in research contexts.

[6] Bender, Emily M. & Hanna, Alex (2021). "The AI Con: How the Tech Industry's Narrow Focus on Deep Learning Impacts Progress in Artificial Intelligence." MIT Press.

[7] Marshall, Barry J. & Warren, J. Robin (1984). "Unidentified Curved Bacilli in the Stomach of Patients with Gastritis and Peptic Ulceration." *The Lancet*. The breakthrough paper that challenged 100+ years of medical consensus on ulcer causes.

[8] Wolf, Thomas (2025, September 15). "Challenging the Average With Open-Source AI." *MIT Sloan Review Audio*. Discussion of AI limitations in novelty generation vs. consensus reinforcement.

---

## ✍️ **Author Bio**

**Brennan Brown** is a Queer Métis writer, technologist, and founder of Write Club, a creative collective for marginalized voices. With an English Honours degree from Mount Royal University and expertise spanning full-stack development to literary analysis, Brennan bridges critical thinking with technical rigor. His published work explores the intersections of technology, culture, and contrarian thought. Based in Calgary, he's committed to asking the questions others avoid.

**[Follow on Twitter](https://twitter.com/brennan) | [Connect on LinkedIn](https://linkedin.com/in/brennan) | [Explore Write Club](https://writeclub.ca)**

---

## 🎯 **What's Next?**

**Read Similar Explorations:**
- [Why AI-Generated Content Is Destroying Workplace Productivity](#related)
- [Why Higher Traffic Doesn't Equal Revenue in Digital Marketing](#related)
- [Why Passive Income Is a Myth](#related)

**Subscribe for Contrarian Insight:**
Join our weekly newsletter where we ask the questions consensus ignores. Plus early access to new articles, reader community, and exclusive research.

[**Subscribe to ThinkingWhy Newsletter →**](#newsletter-cta)

**Disagree? Questions?**  
[**Start a comment below →**](#comments)

---

*Last updated: October 22, 2025 | Reading time: 7 minutes | Originally published: October 22, 2025*

**Share this article:**
- [Share on Twitter](#share-twitter)
- [Share on LinkedIn](#share-linkedin)
- [Share on Facebook](#share-facebook)
- [Copy link](#share-copy)
